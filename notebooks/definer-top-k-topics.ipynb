{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from pyyoutube import Api\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymorphy2\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# handler = logging.FileHandler('../logging/logging.log')\n",
    "# logger.addHandler(handler)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(YOUTUBE_API_KEY, video_id, max_results, next_page_token):\n",
    "    youtube_uri = f'https://www.googleapis.com/youtube/v3/commentThreads?key={YOUTUBE_API_KEY}&textFormat=plainText&' + \\\n",
    "        f'part=snippet&videoId={video_id}&maxResults={max_results}&pageToken={next_page_token}'\n",
    "    \n",
    "    try: \n",
    "        response = requests.get(youtube_uri)\n",
    "        response.raise_for_status()\n",
    "        data = json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, max_results=10, next_page_token=''):\n",
    "    api = Api(api_key=YOUTUBE_API_KEY)\n",
    "    video_by_keywords = api.search_by_keywords(q=query,\n",
    "                                               search_type=[\"video\"],\n",
    "                                               count=count_video,\n",
    "                                               limit=limit\n",
    "    )\n",
    "    video_ids = [x.id.videoId for x in video_by_keywords.items]\n",
    "\n",
    "    comments_all = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY,\n",
    "                            video_id=video_id,\n",
    "                            max_results=max_results,\n",
    "                            next_page_token=next_page_token\n",
    "            )\n",
    "            \n",
    "            if 'items' in data:\n",
    "                comment = list(get_text_of_comment(data))\n",
    "                comments_all.append(comment)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affinity': 'cosine',\n",
       " 'count_max_clusters': 15,\n",
       " 'silhouette_metric': 'euclidean'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = os.path.join('/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/config/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']\n",
    "config['clustering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ð¨Ð¸ÐºÐ¾Ñ, Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€ÑŽ Ð·Ð° Ð²Ð¸Ð´ÐµÐ¾!!\\nÐ’ÑÐµÐ¼ Ð´Ð¾Ð±Ñ€Ð°)',\n",
       " 'ÐÐµÐ¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ð¾Ð¼Ñƒ ÑÐ»ÑƒÑˆÐ°Ñ‚ÐµÐ»ÑŽ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð½ÐµÐ¿Ð¾Ð½ÑÑ‚Ð½Ð¾. \"ÐÐ° Ð²Ñ‹Ñ…Ð¾Ð´Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€...\"',\n",
       " 'ÐŸÐ¾Ð´ÑÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° ÐºÐ°ÐºÐ¾Ð¹ Ñ‚Ð¸Ð¿ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐµÑ‚Ð¸ Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Deep Fake Ð¼ÐµÐ´Ð¸Ð°',\n",
       " 'Ð¿Ð¾Ñ‡Ð°Ñ‰Ðµ Ð¼Ð¾Ñ€Ð³Ð°Ð¹, Ð° Ñ‚Ð¾ Ð²Ñ‹Ð´Ð°ÐµÑˆÑŒ ÑÐ²Ð¾Ð¹ Ð˜Ð˜',\n",
       " 'ÐžÑ‡ÐµÐ½ÑŒ Ð¿Ð¾Ð¼Ð¾Ð³Ð»Ð¾. Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾!',\n",
       " '10 Ð¼Ð¸Ð½ÑƒÑ‚ Ð²Ð¸Ð´ÐµÐ¾ Ð·Ð°Ð¼ÐµÐ½ÑÑ‚ Ð²Ð°Ð¼ 3 Ð¼ÐµÑÑÑ†Ð° ÑÐ½Ð´ÐµÐºÑ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒÐ¼Ð°',\n",
       " 'ÐŸÐ¾ÑÐ¾Ð²ÐµÑ‚ÑƒÐ¹Ñ‚Ðµ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñƒ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°? Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð° ÐºÐ½Ð¸Ð¶ÐºÐ¸ Ñƒ Ð²Ð°Ñ Ð½Ð° ÑÑ‚Ð¾Ð»Ðµ Ñ‚Ð°ÐºÐ¸Ðµ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ðµ? Ð‘Ñ‹Ð»Ð¾ Ð±Ñ‹ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ñ€Ð°Ð·Ð±Ð¾Ñ€ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹ Ð¾Ñ‚ Ð²Ð°Ñ. Ð’Ð¸Ð´ÐµÐ¾ ðŸ”¥ðŸ”¥ðŸ”¥!!!',\n",
       " 'Ð”Ð°Ð²Ð½Ð¾ ÑƒÐ¶Ðµ Ð¿Ñ€Ð¸ÑˆÑ‘Ð» Ðº Ð²Ñ‹Ð²Ð¾Ð´Ñƒ, Ñ‡Ñ‚Ð¾ ÑƒÐ¼ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð²ÐµÑ‰Ð¸ - ÑÑ‚Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°Ðº Ð¾Ñ‡ÐµÐ½ÑŒ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð°.\\nÐŸÐ¾Ð´Ð¿Ð¸ÑÐ°Ð»ÑÑ.\\nÐ•ÑÑ‚ÑŒ ÑˆÐ°Ð»ÑŒÐ½Ð°Ñ Ð¼Ñ‹ÑÐ»ÑŒ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð² ÑÐ²Ð¾ÐµÐ¹ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ (Ñ Ð±Ð¸Ð¾Ð»Ð¾Ð³) - Ð½Ð¾ Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾, Ñ‡Ñ‚Ð¾ ÑÐ°Ð¼Ð¾ Ð¾Ð½Ð¾ Ð½Ðµ ÑÐ´ÐµÐ»Ð°ÐµÑ‚ÑÑ, Ð½Ð°Ð´Ð¾ Ð½ÐµÐ¼Ð°Ð»Ð¾ ÑƒÑÐ¸Ð»Ð¸Ð¹ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ :)',\n",
       " 'Ð­Ñ‚Ð¾ Ð»ÑƒÑ‡ÑˆÐµÐµ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹, Ñ‡Ñ‚Ð¾ Ñ Ð²Ð¸Ð´ÐµÐ» Ð½Ð° Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ€Ð°Ñ… Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ð°. Ð¡Ð¿Ð°ÑÐ¸Ð±Ð¾!',\n",
       " 'ÐœÐµÐ½Ñ Ð²ÑÐµÐ³Ð´Ð° ÑƒÐ´Ð¸Ð²Ð»ÑÑŽÑ‚ Ð»ÑŽÐ´Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ Ð²ÐµÑ‰Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°! Ð›Ð°Ð¹Ðº Ð¸ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ° Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = config['SEED']\n",
    "\n",
    "comments = get_all_comments(**config['comments'])\n",
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "\n",
    "def remove_links(sentence):\n",
    "    link_pattern = r'(http\\S+|bit\\.ly/\\S+|www\\S+)'\n",
    "    sentence = re.sub(link_pattern, '', sentence)\n",
    "    sentence = sentence.strip('[link]')\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def preprocessing(sentence, stop_words, morph):\n",
    "    sentence = remove_emoji(sentence)\n",
    "    sentence = remove_links(sentence)\n",
    "\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    sentence = str_pattern.sub(r'', sentence)\n",
    "\n",
    "    sentence = re.sub('(((?![Ð°-ÑÐ-Ð¯ ]).)+)', ' ', sentence)\n",
    "\n",
    "    sentence = [morph.parse(word)[0].normal_form for word in nltk.word_tokenize(sentence) if word not in stop_words]\n",
    "    sentence = ' '.join(sentence).lower()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_clean_text(data):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    language = config['stopwords']\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    comments = [preprocessing(sentence, stop_words, morph) for sentence in data]\n",
    "    comments = [comm for comm in comments if len(comm) > 2]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    mtx = tfidf.transform(data).toarray()\n",
    "    mask = (np.nan_to_num(mtx) != 0).any(axis=1)\n",
    "    return mtx[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÑˆÐ¸ÐºÐ¾Ñ Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ð¸Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾ Ð²ÐµÑÑŒ Ð´Ð¾Ð±Ñ€Ð¾',\n",
       " 'Ð½ÐµÐ¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ»ÑƒÑˆÐ°Ñ‚ÐµÐ»ÑŒ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð½ÐµÐ¿Ð¾Ð½ÑÑ‚Ð½Ð¾ Ð½Ð° Ð²Ñ‹Ñ…Ð¾Ð´ Ð²ÐµÐºÑ‚Ð¾Ñ€',\n",
       " 'Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° Ñ‚Ð¸Ð¿ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐµÑ‚ÑŒ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¼ÐµÐ´Ð¸Ð°',\n",
       " 'Ñ‡Ð°ÑÑ‚Ñ‹Ð¹ Ð¼Ð¾Ñ€Ð³Ð°Ñ‚ÑŒ Ð²Ñ‹Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ð¹ Ð¸Ñ',\n",
       " 'Ð¾Ñ‡ÐµÐ½ÑŒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ ÑÐ¿Ð°ÑÐ¸Ð±Ð¾',\n",
       " 'Ð¼Ð¸Ð½ÑƒÑ‚Ð° Ð²Ð¸Ð´ÐµÐ¾ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¼ÐµÑÑÑ† ÑÐ½Ð´ÐµÐºÑ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒÐ¼',\n",
       " 'Ð¿Ð¾ÑÐ¾Ð²ÐµÑ‚Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ ÐºÐ½Ð¸Ð¶ÐºÐ° ÑÑ‚Ð¾Ð» Ñ‚Ð°ÐºÐ¾Ð¹ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¹ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ñ€Ð°Ð·Ð±Ð¾Ñ€ Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð²Ð¸Ð´ÐµÐ¾',\n",
       " 'Ð´Ð°Ð²Ð½Ð¾ Ð¿Ñ€Ð¸ÑˆÐ° Ð» Ð²Ñ‹Ð²Ð¾Ð´ ÑƒÐ¼ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ð¹ Ð²ÐµÑ‰ÑŒ ÑÑ‚Ð¾ Ð¿Ñ€Ð¸Ð·Ð½Ð°Ðº Ð¾Ñ‡ÐµÐ½ÑŒ Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¸Ð¹ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ°Ñ‚ÑŒÑÑ ÐµÑÑ‚ÑŒ ÑˆÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ñ‹ÑÐ»ÑŒ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ²Ð¾Ð¹ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð±Ð¸Ð¾Ð»Ð¾Ð³ Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾ ÑÐ°Ð¼ Ð¾Ð½Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒÑÑ Ð½ÐµÐ¼Ð°Ð»Ð¾ ÑƒÑÐ¸Ð»Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ',\n",
       " 'ÑÑ‚Ð¾ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð½ÐµÐ¹Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ ÑÐµÑ‚ÑŒ Ð²Ð¸Ð´ÐµÑ‚ÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ€ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚ ÑÐ¿Ð°ÑÐ¸Ð±Ð¾',\n",
       " 'Ñ ÑƒÐ´Ð¸Ð²Ð»ÑÑ‚ÑŒ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ñ‡ÑŒ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ð¹ Ð²ÐµÑ‰ÑŒ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐ»Ð¾Ð²Ð¾ Ð»Ð°Ð¹Ðº Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ° Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_comments = get_clean_text(comments)\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(cleaned_comments)\n",
    "cleaned_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2741, 300),\n",
       " array(['Ð°Ð²Ñ‚Ð¾Ñ€', 'Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ', 'Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼', 'Ð±Ð°Ð·Ð°', 'Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ð¸Ñ‚ÑŒ', 'Ð±Ð»Ð¸Ð½',\n",
       "        'Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¹', 'Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹', 'Ð±ÑƒÐ´ÑƒÑ‰ÐµÐµ', 'Ð±Ñ‹Ñ‚ÑŒ'], dtype=object))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx = vectorize_text(cleaned_comments, tfidf)\n",
    "mtx.shape, tfidf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity, silhouette_metric):\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric)\n",
    "        )\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    if len(unique_cluster_labels) > 2:\n",
    "        return f1_score(y_test, y_pred, average='macro')\n",
    "    else:\n",
    "        return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  0,  6, 13, 12, 11,  0,  0, 12,  0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels = get_clusters(mtx, random_state=SEED, **config['clustering'])\n",
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mtx, cluster_labels, **config['cross_val'], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/10 22:59:21 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'vector_tfidf' already exists. Creating a new version of this model...\n",
      "2024/05/10 22:59:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: vector_tfidf, version 10\n",
      "Created version '10' of model 'vector_tfidf'.\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'LogisticRegression' already exists. Creating a new version of this model...\n",
      "2024/05/10 22:59:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 10\n",
      "Created version '10' of model 'LogisticRegression'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://arts/2/44fedd14c6224147bbbf8fc334bfbe52/artifacts'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test), set(cluster_labels))\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\"\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\"\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    dict_push = {}\n",
    "    model_versions = client.search_model_versions(f\"name='{config_name}'\")\n",
    "    for count, value in enumerate(model_versions):\n",
    "        # client.list_registered_models()):\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', '1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)\n",
    "last_version_lr, last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
