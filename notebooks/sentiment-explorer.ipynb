{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aegon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from pyyoutube import Api\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "\n",
    "# handler = logging.FileHandler('../logging/logging.log')\n",
    "# logger.addHandler(handler)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(YOUTUBE_API_KEY, video_id, max_results, next_page_token):\n",
    "    youtube_uri = f'https://www.googleapis.com/youtube/v3/commentThreads?key={YOUTUBE_API_KEY}&textFormat=plainText&' + \\\n",
    "        f'part=snippet&videoId={video_id}&maxResults={max_results}&pageToken={next_page_token}'\n",
    "    \n",
    "    try: \n",
    "        response = requests.get(youtube_uri)\n",
    "        response.raise_for_status()\n",
    "        data = json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, max_results=10, next_page_token=''):\n",
    "    api = Api(api_key=YOUTUBE_API_KEY)\n",
    "    video_by_keywords = api.search_by_keywords(q=query,\n",
    "                                               search_type=[\"video\"],\n",
    "                                               count=count_video,\n",
    "                                               limit=limit\n",
    "    )\n",
    "    video_ids = [x.id.videoId for x in video_by_keywords.items]\n",
    "\n",
    "    comments_all = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY,\n",
    "                            video_id=video_id,\n",
    "                            max_results=max_results,\n",
    "                            next_page_token=next_page_token\n",
    "            )\n",
    "            \n",
    "            if 'items' in data:\n",
    "                comment = list(get_text_of_comment(data))\n",
    "                comments_all.append(comment)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affinity': 'cosine',\n",
       " 'count_max_clusters': 15,\n",
       " 'silhouette_metric': 'euclidean'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = os.path.join('/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/config/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']\n",
    "config['clustering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = config['SEED']\n",
    "\n",
    "comments = get_all_comments(**config['comments'])\n",
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(sentence):\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "\n",
    "def remove_links(sentence):\n",
    "    link_pattern = r'(http\\S+|bit\\.ly/\\S+|www\\S+)'\n",
    "    sentence = re.sub(link_pattern, '', sentence)\n",
    "    sentence = sentence.strip('[link]')\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def preprocessing(sentence, stop_words, lemmatizer):\n",
    "    sentence = remove_emoji(sentence)\n",
    "    sentence = remove_links(sentence)\n",
    "\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    sentence = str_pattern.sub(r'', sentence)\n",
    "\n",
    "    sentence = re.sub('(((?![а-яА-Я ]).)+)', ' ', sentence)\n",
    "\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(sentence) if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_clean_text(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    language = config['stopwords']\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    comments = [preprocessing(sentence, stop_words, lemmatizer) for sentence in data]\n",
    "    comments = [comm for comm in comments if len(comm) > 2]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    mtx = tfidf.transform(data).toarray()\n",
    "    mask = (np.nan_to_num(mtx) != 0).any(axis=1)\n",
    "    return mtx[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Неплохой видос',\n",
       " 'И статья',\n",
       " 'На самом деле задача поиска аномалий самая простая перечисленных Если проснулись ноябре месяце рано утром г Москве улице аномалия А серьезно аномалии разыскиваются среднему отклонению среднего Причем важен модуль аномалии размер То ноября г Москвы это такая крутая аномалия',\n",
       " 'якобы вычислительные мощности выросли нейронки поднялись это бред полнейший реальности нейронки поднялись открытия сверточных нейронных сетей',\n",
       " 'ролик спасибо кдинственное замечание очень понятно различие классификацией кластеризацией',\n",
       " 'Большое спасибо',\n",
       " 'Спасибо Сделал несколько выводов студента',\n",
       " 'Спасибо качественный контент',\n",
       " 'Разве значение нейрона предыдущего слоя складывается значением ребра весом умножается',\n",
       " 'От души ссылки рассказ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_comments = get_clean_text(comments)\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(cleaned_comments)\n",
    "cleaned_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1912, 300),\n",
       " array(['автор', 'автору', 'библиотеки', 'благодарю', 'блин', 'большое',\n",
       "        'буду', 'будут', 'быстро', 'ваш'], dtype=object))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx = vectorize_text(cleaned_comments, tfidf)\n",
    "mtx.shape, tfidf.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity, silhouette_metric):\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric)\n",
    "        )\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    if len(unique_cluster_labels) > 2:\n",
    "        return f1_score(y_test, y_pred, average='macro')\n",
    "    else:\n",
    "        return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  1,  1, 12, 12, 12, 12, 12, 12, 12], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels = get_clusters(mtx, random_state=SEED, **config['clustering'])\n",
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mtx, cluster_labels, **config['cross_val'], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/23 14:03:01 INFO mlflow.tracking.fluent: Experiment with name 'first' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/23 14:03:02 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Successfully registered model 'vector_tfidf'.\n",
      "2024/04/23 14:03:05 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: vector_tfidf, version 1\n",
      "Created version '1' of model 'vector_tfidf'.\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.11/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/.venv/lib64/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'LogisticRegression'.\n",
      "2024/04/23 14:03:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LogisticRegression, version 1\n",
      "Created version '1' of model 'LogisticRegression'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/aegon/Documents/Development/Projects/topic-sentiment-explorer/mlflow/1/fc8352c20562487cb198f608d9a62d48/artifacts'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test), set(cluster_labels))\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\"\n",
    "    )\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\"\n",
    "    )\n",
    "    mlflow.end_run()\n",
    "\n",
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    dict_push = {}\n",
    "    model_versions = client.search_model_versions(f\"name='{config_name}'\")\n",
    "    for count, value in enumerate(model_versions):\n",
    "        # client.list_registered_models()):\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', '1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)\n",
    "last_version_lr, last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
